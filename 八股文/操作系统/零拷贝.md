## 零拷贝

没有在内存层面去拷贝数据，即全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。

### 传统的文件 I/O

- 需要**4 次用户态与内核态的上下文切换**。发生了两次系统调用，一次是 `read()` ，一次是 `write()`，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。    
- 需要4 次数据拷贝，其中两次是 DMA 的拷贝（磁盘和内核缓冲区之间），另外两次则是通过 CPU 拷贝的（内核缓冲区和用户缓冲区）

![j8IjfY](https://cdn.jsdelivr.net/gh/Maxaayang/pic@main/uPic/j8IjfY.png)

### 零拷贝实现方式
#### mmap + write

在前面我们知道，`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 `mmap()` 替换 `read()` 系统调用函数。mmap直接将文件映射，不需要用read读。

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

我们可以得知，通过使用 `mmap()` 来代替 `read()`， 可以减少一次数据拷贝的过程。**仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。**

![KEGudT](https://cdn.jsdelivr.net/gh/Maxaayang/pic@main/uPic/KEGudT.png)


#### sendfile

```Apache
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

前两个参数分别是目的端和源端的文件描述符，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以**减少一次系统调用，也就减少了 2 次上下文切换的开销。**

可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。

![mUDFEI](https://cdn.jsdelivr.net/gh/Maxaayang/pic@main/uPic/mUDFEI.png)

#### SG-DMA

如果网卡支持 SG-DMA（_The Scatter-Gather Direct Memory Access_）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程

`sendfile()` 系统调用的过程发生了点变化，具体过程如下：
- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，将内核空间的读缓冲区中对应的数据描述信息(内存地址、地址偏移量)记录到相应的 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以根据内存地址、地址偏移量直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

2次上下文切换、0次CPU拷贝、2次DMA拷贝

![0V386Y](https://cdn.jsdelivr.net/gh/Maxaayang/pic@main/uPic/0V386Y.png)

#### splice

在内核空间的读缓冲区和网络缓冲区之间建立管道，从而避免了两者之间的CPU拷贝操作，2次上下文切换，0次CPU拷贝以及2次DMA拷贝。

![](https://pic4.zhimg.com/v2-437bf99c54a064faedb89a680ab3e0c3_r.jpg)



> 参考文献
>
> https://mp.weixin.qq.com/s/j1KjR5bRQV6oGrU5tsyHgg
>
> https://zhuanlan.zhihu.com/p/565012118

